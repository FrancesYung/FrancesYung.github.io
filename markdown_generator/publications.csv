pub_date	year	authors	citation	category	title	venue	excerpt	url_slug	paper_url	slides_url
2013-01-01	2013	Yutaro Shigeto, Ai  Azuma, Sorami Hisamoto, Shuhei Kondo, Tomoya Kose, Keisuke Sakaguchi, Akifumi Yoshimoto, Frances Yung, Yuji Matsumoto	Yutaro Shigeto, Ai  Azuma, Sorami Hisamoto, Shuhei Kondo, Tomoya Kose, Keisuke Sakaguchi, Akifumi Yoshimoto, Frances Yung, Yuji Matsumoto (Workshop on Multiword Expressions @ NAACL 2013)	conferences	Construction of English MWE dictionary and its application to POS tagging	Workshop on Multiword Expressions @ NAACL	This paper reports our ongoing project for constructing an English multiword expression (MWE) dictionary and NLP tools based on the developed dictionary. We extracted functional MWEs from the English part of Wiktionary, annotated the Penn Treebank (PTB) with MWE information, and conducted POS tagging experiments. We report how the MWEannotation is done on PTB and the results of POS and MWE tagging experiments.	Construction-of-english-mwe	https://aclanthology.org/W13-1021.pdf	
2014-01-01	2014	Frances Yung	Frances Yung (Student Research Workshop @ ACL 2014)	conferences	Towards a discourse relation-aware approach for Chinese-English machine translation	Student Research Workshop @ ACL	Translation of discourse relations is one of the recent efforts of incorporating discourse information to statistical machine translation (SMT). While existing works focus on disambiguation of ambiguous discourse connectives, or transformation of discourse trees, only explicit discourse relations are tackled. A greater challenge exists in machine translation of Chinese, since implicit discourse relations are abundant and occur both inside and outside a sentence. This thesis proposal describes ongoing workonbilingual discourse annotation and plans towards incorporating discourse relation knowledge to a ChineseEnglish SMTsystemwithconsideration of implicit discourse relations. The final goal is a discourse-unit-based translation model unbounded by the traditional assumption of sentence-to-sentence translation.	Towards-a-discourse-relation-aware	https://aclanthology.org/P14-3003.pdf	
2014-01-01	2014	Frances Yung, Kevin Duh, Yuji Matsumoto	Frances Yung, Kevin Duh, Yuji Matsumoto (EACL 2014)	conferences	Analysis and Prediction of Unalignable Words in Parallel Text	EACL	Professional human translators usually do not employ the concept of word alignments, producing translations ‘sense-forsense’ instead of ‘word-for-word’. This suggests that unalignable words may be prevalent in the parallel text used for machine translation (MT). We analyze this phenomenonin-depth for Chinese-English translation. We further propose a simple and effective method to improve automatic word alignment by pre-removing unalignable words, and show improvements on hierarchical MT systems in both translation directions.	Analysis-and-prediction-of-unalignable	https://aclanthology.org/E14-4037.pdf	
2015-01-01	2015	Enrico Santus, Frances Yung, Alessandro Lenci, Chu-Ren Huang	Enrico Santus, Frances Yung, Alessandro Lenci, Chu-Ren Huang (Workshop on Linked Data in Linguistics @ ACL 2015)	conferences	Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models	Workshop on Linked Data in Linguistics @ ACL	In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1. An extension in RDFformat, including also scripts for data processing, is under development.	evaluation-1.0	https://aclanthology.org/W15-4208.pdf	
2015-01-01	2015	Frances Yung, Kevin Duh, Yuji Matsumoto	Frances Yung, Kevin Duh, Yuji Matsumoto (SIGHAN Workshop on Chinese Language Processing @ ACL 2015)	conferences	Sequential annotation and chunking of Chinese discourse structure	SIGHAN Workshop on Chinese Language Processing @ ACL	We propose a linguistically driven approach to represent discourse relations in Chinese text as sequences. We observe that certain surface characteristics of Chinese texts, such as the order of clauses, are overt markers of discourse structures, yet existing annotation proposals adapted from formalism constructed for English do not fully incorporate these characteristics. We present an annotated resource consisting of 325 articles in the Chinese Treebank. In addition, using this annotation, we introduce a discourse chunker based on a cascade of classifiers and report 70% top-level discourse sense accuracy.	Sequential-annotation-and-chunking-of-chinese	https://aclanthology.org/W15-3101.pdf	
2015-01-01	2015	Frances Yung, Kevin Duh, Yuji Matsumoto	Frances Yung, Kevin Duh, Yuji Matsumoto (Workshop on Discourse in Machine Translation @ EMNLP 2015)	conferences	Crosslingual annotation and analysis of implicit discourse connectives for machine translation	Workshop on Discourse in Machine Translation @ EMNLP	Usage of discourse connectives (DCs) differs across languages, thus addition and omission of connectives are common in translation. We investigate how implicit (omitted) DCs in the source text impacts various machine translation (MT) systems, and whether a discourse parser is needed as a preprocessor to explicitate implicit DCs. Based on the manual annotation and alignment of 7266 pairs of discourse relations in a Chinese-English translation corpus, we evaluate whether a preprocessing step that inserts explicit DCs at positions of implicit relations can improve MT. Results show that, without modifying the translation model, explicitating implicit relations in the input source text has limited effect on MT evaluation scores. In addition, translation spotting analysis shows that it is crucial to identify DCs that should be explicitly translated in order to improve implicit-to-explicit DC translation. Onthe other hand, further analysis reveals that the disambiguation as well as explicitation of implicit relations are subject to a certain level of optionality, suggesting the limitation to learn and evaluate this linguistic phenomenon using standard parallel corpora.	Crosslingual-annotation-and-analysis	https://aclanthology.org/W15-2519.pdf	
2016-01-01	2016	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto (ACL 2016)	conferences	Modeling the interpretation of discourse connectives by Bayesian pragmatics	ACL	We propose a framework to model human comprehension of discourse connectives. Following the Bayesian pragmatic paradigm, we advocate that discourse connectives are interpreted based on a simulation of the production process by the speaker, who, in turn, considers the ease of interpretation for the listener when choosing connectives. Evaluation against the sense annotation of the Penn Discourse Treebank confirms the superiority of the model over literal comprehension. A further experiment demonstrates that the proposed model also improves automatic discourse parsing.	Modeling-the-interpretation-of-discourse	https://aclanthology.org/P16-2086.pdf	
2016-01-01	2016	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto (CoNLL @ ACL 2016)	conferences	Modelling the usage of discourse connectives as rational speech acts	CoNLL @ ACL	Discourse relations can either be implicit or explicitly expressed by markers, such as ’therefore’ and ’but’. How a speaker makes this choice is a question that is not well understood. We propose a psycholinguistic model that predicts whether a speaker will produce an explicit marker given the discourse relation s/he wishes to express. Based on the framework of the Rational Speech Acts model, we quantify the utility of producing a marker based on the information-theoretic measure of surprisal, the cost of production, and a bias to maintain uniform information density throughout the utterance. Experiments based on the Penn Discourse Treebank show that our approach outperforms stateof-the-art approaches, while giving an explanatory account of the speaker’s choice.	Modelling-the-usage-of-discourse-connective	https://aclanthology.org/K16-1030.pdf	
2017-01-01	2017	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto	Frances Yung, Kevin Duh, Taku Komura, Yuji Matsumoto (Dialogue and Discourse 2017)	manuscripts	A psycholinguistic model for the marking of discourse relations	Dialogue and Discourse	Discourse relations can either be explicitly marked by discourse connectives (DCs), such as therefore and but, or implicitly conveyed in natural language utterances. How speakers choose between the two options is a question that is not well understood. In this study, we propose a psycholinguistic model that predicts whether or not speakers will produce an explicit marker given the discourse relation they wish to express. Our model is based on two information-theoretic frameworks: (1) the Rational Speech Acts model, which models the pragmatic interaction between language production and interpretation by Bayesian inference, and (2) the Uniform Information Density theory, which advocates that speakers adjust linguistic redundancy to maintain a uniform rate of information transmission. Specifically, our model quantifies the utility of using or omitting a DC based on the expected surprisal of comprehension, cost of production, and availability of other signals in the rest of the utterance. Experiments based on the Penn Discourse Treebank show that our approach outperforms the state-of-the-art performance at predicting the presence of DCs (Patterson and Kehler, 2013), in addition to giving an explanatory account of the speaker's choice.	Psycholinguistic-model-for-the-marking	https://journals.uic.edu/ojs/index.php/dad/article/view/10685/9460	
2017-01-01	2017	Frances Yung, Hiroshi Noji, Yuji Matsumoto	Frances Yung, Hiroshi Noji, Yuji Matsumoto (IJPNLP 2017)	conferences	Can Discourse Relations be Identified Incrementally?	IJPNLP	Humans process language word by word and construct partial linguistic structures on the fly before the end of the sentence is perceived. Inspired by this cognitive ability, incremental algorithms for natural language processing tasks have been proposed and demonstrated promising performance. For discourse relation (DR) parsing, however, it is not yet clear to what extent humans can recognize DRs incrementally, because the latent ‘nodes’ of discourse structure can span clauses and sentences. To answer this question, this work investigates incrementality in discourse processing based on a corpus annotated with DR signals. We find that DRsaredominantly signaled at the boundary between the two constituent discourse units. The findings complement existing psycholinguistic theories on expectation in discourse processing and provide direction for incremental discourse parsing.	Can-discourse-relations-be-identified-incrementally	https://aclanthology.org/I17-2027/	
2017-01-01	2017	Wei Shi, Frances Yung, Raphael Rubino, Vera Demberg	Wei Shi, Frances Yung, Raphael Rubino, Vera Demberg (IJPNLP 2017)	conferences	Using explicit discourse connectives in translation for implicit discourse relation classification	IJPNLP	Implicit discourse relation recognition is an extremely challenging task due to the lack of indicative connectives. Various neural network architectures have been proposed for this task recently, but most of them suffer from the shortage of labeled data. In this paper, we address this problem by procuring additional training data from parallel corpora: When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.	Using-explicit-discourse-connectives-in-translation	https://aclanthology.org/I17-1049/	
2018-01-01	2018	Frances Yung, Vera Demberg	Frances Yung, Vera Demberg (Workshop on Cognitive Aspects of Computational Language Learning and Processing @ ACL 2018)	conferences	Do speakers produce discourse connectives rationally?	Workshop on Cognitive Aspects of Computational Language Learning and Processing @ ACL	A number of different discourse connectives can be used to mark the same discourse relation, but it is unclear what factors affect connective choice. One recent account is the Rational Speech Acts theory, which predicts that speakers try to maximize the informativeness of an utterance such that the listener can interpret the intended meaning correctly. Existing prior work uses referential language games to test the rational account of speakers’ production of concrete meanings, such as identification of objects within a picture. Building on the same paradigm, we design a novel Discourse Continuation Game to investigate speakers’ production of abstract discourse relations. Experimental results reveal that speakers significantly prefer a more informative connective, in line with predictions of the RSA model.	Do-speakers-produce-discourse-connectives	https://aclanthology.org/W18-2802/	
2018-01-01	2018	Wei Shi, Frances Yung, Vera Demberg	Wei Shi, Frances Yung, Vera Demberg (Workshop on Discourse Relation Parsing and Treebanking @ NAACL 2018)	conferences	Acquiring annotated data with cross-lingual explicitation for implicit discourse relation classification	Workshop on Discourse Relation Parsing and Treebanking @ NAACL	Implicit discourse relation classification is one of the most challenging and important tasks in discourse parsing, due to the lack of connectives as strong linguistic cues. A principle bottleneck to further improvement is the shortage of training data (ca. 18k instances in the Penn Discourse Treebank (PDTB)). Shi et al. (2017) proposed to acquire additional data by exploiting connectives in translation: human translators mark discourse relations which are implicit in the source language explicitly in the translation. Using back-translations of such explicitated connectives improves discourse relation parsing performance. This paper addresses the open question of whether the choice of the translation language matters, and whether multiple translations into different languages can be effectively used to improve the quality of the additional data.	Aquiring-annotated-data-with-cross-lingual	https://aclanthology.org/W19-2703.pdf	
2020-01-01	2020	Frances Yung, Jana Jungbluth, Vera Demberg	Frances Yung, Jana Jungbluth, Vera Demberg (Societas Linguistica Europaea 2020)	conferences	Modeling the interplay of rational production and comprehension of ambiguous connectives	Societas Linguistica Europaea		Modeling-the-interplay-of-rational-production	http://FrancesYung.github.io/files/SLE2020_W11.pdf	
2019-01-01	2019	Frances Yung, Merel Scholman, Vera Demberg	Frances Yung, Merel Scholman, Vera Demberg (Linguistic Annotation Workshop @ ACL 2019)	conferences	Crowdsourcing discourse relation annotations by a two-step connective insertion task	Linguistic Annotation Workshop @ ACL	The perspective of being able to crowd-source coherence relations bears the promise of acquiring annotations for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions: Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating coherence relations with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from connectives that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and annotations from PDTB and RSTDT on the same texts.	Crowdsourcing-discourse-relation-annotations	https://aclanthology.org/W19-4003.pdf	
2021-01-01	2021	Frances Yung, Jana Jungbluth, Vera Demberg	Frances Yung, Jana Jungbluth, Vera Demberg (Frontiers in Psychology 2021)	manuscripts	Limits to the rational production of discourse connectives	Frontiers in Psychology	Rational accounts of language use such as the uniform information density hypothesis, which asserts that speakers distribute information uniformly across their utterances, and the rational speech act (RSA) model, which suggests that speakers optimize the formulation of their message by reasoning about what the comprehender would understand, have been hypothesized to account for a wide range of language use phenomena. We here specifically focus on the production of discourse connectives. While there is some prior work indicating that discourse connective production may be governed by RSA, that work uses a strongly gamified experimental setting. In this study, we aim to explore whether speakers reason about the interpretation of their conversational partner also in more realistic settings. We thereby systematically vary the task setup to tease apart effects of task instructions and effects of the speaker explicitly seeing the interpretation alternatives for the listener. Our results show that the RSA-predicted effect of connective choice based on reasoning about the listener is only found in the original setting where explicit interpretation alternatives of the listener are available for the speaker. The effect disappears when the speaker has to reason about listener interpretations. We furthermore find that rational effects are amplified by the gamified task setting, indicating that meta-reasoning about the specific task may play an important role and potentially limit the generalizability of the found effects to more naturalistic every-day language use.	Limits-to-the-rational-production-of-discourse	https://www.frontiersin.org/articles/10.3389/fpsyg.2021.660730/full	
2021-01-01	2021	Merel Scholman, Tianai  Dong, Frances Yung, Vera Demberg	Merel Scholman, Tianai  Dong, Frances Yung, Vera Demberg (Workshop on Computational Approaches to Discourse@ EMNLP 2021)	conferences	Comparison of methods for explicit discourse connective identification across various domains	Workshop on Computational Approaches to Discourse@ EMNLP	Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang et al., 2015; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other parse methods in all datasets. However, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.	Comparison-of-methods-for-explicit-discourse	https://aclanthology.org/2021.codi-main.9/	
2021-01-01	2021	Frances Yung, Merel Scholman, Vera Demberg	Frances Yung, Merel Scholman, Vera Demberg (Workshop on Computational Approaches to Discourse@ EMNLP 2021)	conferences	A practical perspective on connective generation	Workshop on Computational Approaches to Discourse@ EMNLP	In data-driven natural language generation, we typically know what relation should be expressed and need to select a connective to lexicalize it. In the current contribution, we analyse whether a sophisticated connective generation module is necessary to select a connective, or whether this can be solved with simple methods (such as random choice between connectives that are known to express a given relation, or usage of a generic language model). Comparing these methods to the distributions of connective choices from a human connective insertion task, we find mixed results: for some relations, it is acceptable to lexicalize them using any of the connectives that mark this relation. However, for other relations (temporals, concessives) either a more detailed relation distinction needs to be introduced, or a more sophisticated connective choice module would be necessary.	Practical-perspective-on-connective-generation	https://aclanthology.org/2021.codi-main.7/	
2022-01-01	2022	Merel Scholman, Tianai  Dong, Frances Yung, Vera Demberg	Merel Scholman, Tianai  Dong, Frances Yung, Vera Demberg (LREC 2022)	conferences	Discogem: A crowdsourced corpus of genre-mixed implicit discourse relations	LREC	We present DiscoGeM, a crowdsourced corpus of 6,505 implicit discourse relations from three genres: political speech, literature, and encyclopedic texts. Each instance was annotated by 10 crowd workers. Various label aggregation methods were explored to evaluate how to obtain a label that best captures the meaning inferred by the crowd annotators. The results show that a significant proportion of discourse relations in DiscoGeM are ambiguous and can express multiple relation senses. Probability distribution labels better capture these interpretations than single labels. Further, the results emphasize that text genre crucially affects the distribution of discourse relations, suggesting that genre should be included as a factor in automatic relation classification. We make available the newly created DiscoGeM corpus, as well as the dataset with all annotator-level labels. Both the corpus and the dataset can facilitate a multitude of applications and research purposes, for example to function as training data to improve the performance of automatic discourse relation parsers, as well as facilitate research into non-connective signals of discourse relations.	Discogem-a-crowdsourced-corpus	http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.351.pdf	
2022-01-01	2022	Merel Scholman, Valentina Pyatkin, Frances Yung, Ido Dagan, Reut Tsarfaty, Vera Demberg	Merel Scholman, Valentina Pyatkin, Frances Yung, Ido Dagan, Reut Tsarfaty, Vera Demberg (LREC 2022)	conferences	Design choices in crowdsourcing discourse relation annotations: The effect of worker selection and training	LREC	Obtaining linguistic annotation from novice crowdworkers is far from trivial. A case in point is the annotation of discourse relations, which is a complicated task. Recent methods have obtained promising results by extracting relation labels from either discourse connectives (DCs) or question-answer (QA) pairs that participants provide. The current contribution studies the effect of worker selection and training on the agreement on implicit relation labels between workers and gold labels, for both the DC and the QA method. In Study 1, workers were not specifically selected or trained, and the results show that there is much room for improvement. Study 2 shows that a combination of selection and training does lead to improved results, but the method is cost- and time-intensive. Study 3 shows that a selection-only approach is a viable alternative; it results in annotations of comparable quality compared to annotations from trained participants. The results generalized over both the DC and QA method and therefore indicate that a selection-only approach could also be effective for other crowdsourced discourse annotation tasks.	Design-choice-in-crowdsourcing-discourse	https://aclanthology.org/2022.lrec-1.231/	
2022-01-01	2022	Frances Yung, Kaveri Anuranjana, Merel Scholman, Vera Demberg	Frances Yung, Kaveri Anuranjana, Merel Scholman, Vera Demberg (Workshop on Computational Approaches to Discourse@ COLING 2022)	conferences	Label distributions help implicit discourse relation classification	Workshop on Computational Approaches to Discourse@ COLING	Implicit discourse relations can convey more than one relation sense, but much of the research on discourse relations has focused on single relation senses. Recently, DiscoGeM, a novel multi-domain corpus, which contains 10 crowd-sourced labels per relational instance, has become available. In this paper, we analyse the co-occurrences of relations in DiscoGem and show that they are systematic and characteristic of text genre. We then test whether information on multi-label distributions in the data can help implicit relation classifiers. Our results show that incorporating multiple labels in parser training can improve its performance, and yield label distributions which are more similar to human label distributions, compared to a parser that is trained on just a single most frequent label per instance.	Label-distributions-help-implicit-discourse	https://aclanthology.org/2022.codi-1.7/	
2022-01-01	2022	Marian Marchal, Merel Scholman, Frances Yung, Vera Demberg	Marian Marchal, Merel Scholman, Frances Yung, Vera Demberg (COLING 2022)	conferences	Establishing annotation quality in multi-label annotations	COLING	In many linguistic fields requiring annotated data, multiple interpretations of a single item are possible. Multi-label annotations more accurately reflect this possibility. However, allowing for multi-label annotations also affects the chance that two coders agree with each other. Calculating inter-coder agreement for multi-label datasets is therefore not trivial. In the current contribution, we evaluate different metrics for calculating agreement on multi-label annotations: agreement on the intersection of annotated labels, an augmented version of Cohen’s Kappa, and precision, recall and F1. We propose a bootstrapping method to obtain chance agreement for each measure, which allows us to obtain an adjusted agreement coefficient that is more interpretable. We demonstrate how various measures affect estimates of agreement on simulated datasets and present a case study of discourse relation annotations. We also show how the proportion of double labels, and the entropy of the label distribution, influences the measures outlined above and how a bootstrapped adjusted agreement can make agreement measures more comparable across datasets in multi-label scenarios.	Establishing-annotation-quality-in-multi	https://aclanthology.org/2022.coling-1.322/	
2023-01-01	2023	Frances Yung, Merel Scholman, Ekaterina Lapshinova-Koltunski, Christina Pollkläsener, Vera Demberg	Frances Yung, Merel Scholman, Ekaterina Lapshinova-Koltunski, Christina Pollkläsener, Vera Demberg (DGfS 2023)	conferences	Translation Effect on Discourse Connective Choice	DGfS		Translation-effect-on-discourse-connective	http://FrancesYung.github.io/files/SLE2020_W11.pdfDGfS_DC_alignment	
2023-01-01	2023	Nobel Varghese, Frances Yung, Kaveri Anuranjana, Vera Demberg	Nobel Varghese, Frances Yung, Kaveri Anuranjana, Vera Demberg (Workshop on Computational Approaches to Discourse@ ACL 2023)	conferences	Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation Classification	Workshop on Computational Approaches to Discourse@ ACL	In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another on the contrary, there are several frameworks that describe the labels’ similarities (by e.g. sorting them into a hierarchy or describing them interms of features (Sanders et al., 2021)). Recently, several methods for representing the similarities between labels have been proposed (Zhang et al., 2018; Wang et al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion Model (Guo et al., 2021) for learning a representation for discourse relation labels. We explore alternative ways of informing the model about the similarities between relations, by representing relations in terms of their names (and parent category), their typical markers, or in terms of CCR features that describe the relations. Experimental results show that exploiting label similarity improves classification results.	Exploiting-knowledge-about-discourse-relation	https://aclanthology.org/2023.codi-1.13/	
2023-01-01	2023	Frances Yung, Merel Scholman, Ekaterina Lapshinova-Koltunski, Christina Pollkläsener, Vera Demberg	Frances Yung, Merel Scholman, Ekaterina Lapshinova-Koltunski, Christina Pollkläsener, Vera Demberg (SIGDIAL 2023)	conferences	Investigating Explicitation of Discourse Connectives in Translation using Automatic Annotations	SIGDIAL	Discourse relations have different patterns of marking across different languages. As a result, discourse connectives are often added, omitted, or rephrased in translation. Prior work has shown a tendency for explicitation of discourse connectives, but such work was conducted using restricted sample sizes due to difficulty of connective identification and alignment. The current study exploits automatic methods to facilitate a large-scale study of connectives in English and German parallel texts. Our results based on over 300 types and 18000 instances of aligned connectives and an empirical approach to compare the cross-lingual specificity gap provide strong evidence of the Explicitation Hypothesis. We conclude that discourse relations are indeed more explicit in translation than texts written originally in the same language. Automatic annotations allow us to carry out translation studies of discourse relations on a large scale. Our methodology using relative entropy to study the specificity of connectives also provides more fine-grained insights into translation patterns.	Investigating-explicitation-of-discourse-connective	https://aclanthology.org/2023.sigdial-1.2/	
2023-01-01	2023	Valentina Pyatkin, Frances Yung, Merel Scholman, Ido Dagan, Reut Tsarfaty, Vera Demberg	Valentina Pyatkin, Frances Yung, Merel Scholman, Ido Dagan, Reut Tsarfaty, Vera Demberg (TACL 2023)	manuscripts	Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases introduced by Task Design	TACL	Disagreement in natural language annotation has mostly been studied from a perspective of biases introduced by the annotators and the annotation frameworks. Here, we propose to analyze another source of bias—task design bias, which has a particularly strong impact on crowdsourced linguistic annotations where natural language is used to elicit the interpretation of lay annotators. For this purpose we look at implicit discourse relation annotation, a task that has repeatedly been shown to be difficult due to the relations’ ambiguity. We compare the annotations of 1,200 discourse relations obtained using two distinct annotation tasks and quantify the biases of both methods across four different domains. Both methods are natural language annotation tasks designed for crowdsourcing. We show that the task design can push annotators towards certain relations and that some discourse relation senses can be better elicited with one or the other annotation approach. We also conclude that this type of bias should be taken into account when training and testing models.	Design-choices-for-crowdsourcing-implicit	https://virtual2023.aclweb.org/paper_T4803.html	
2024-01-01	2024	Frances Yung, Mansoor Ahmad, Merel Scholman, Vera Demberg	Frances Yung, Mansoor Ahmad, Merel Scholman, Vera Demberg (Linguistic Annotation Workshop @ EACL 2024)	conferences	Prompting Implicit Discourse Relation Annotation	Linguistic Annotation Workshop @ EACL	Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT’s performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT’s recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.	Prompting-implicit-discourse-relation	https://aclanthology.org/2024.law-1.pdf#page=162	
2024-01-01	2024	Frances Yung, Merel Scholman, Šárka Zikánová, Vera Demberg	Frances Yung, Merel Scholman, Šárka Zikánová, Vera Demberg (LREC-COLING 2024)	conferences	DiscoGeM 2.0: A Parallel Corpus of English, German, French and Czech Implicit Discourse Relations	LREC-COLING	We present DiscoGeM 2.0, a crowdsourced, parallel corpus of 12,834 implicit discourse relations, with English, German, French and Czech data. We propose and validate a new single-step crowdsourcing annotation method and apply it to collect new annotations in German, French and Czech. The corpus was constructed by having crowdsourced annotators choose a suitable discourse connective for each relation from a set of unambiguous candidates. Every instance was annotated by 10 workers. Our corpus hence represents the first multi-lingual resource that contains distributions of discourse interpretations for implicit relations. The results show that the connective insertion method of discourse annotation can be reliably extended to other languages. The resulting multi-lingual annotations also reveal that implicit relations inferred in one language may differ from those inferred in the translation, meaning the annotations are not always directly transferable. DiscoGem 2.0 promotes the investigation of cross-linguistic differences in discourse marking and could improve automatic discourse parsing applications. It is openly downloadable here: https://github.com/merelscholman/DiscoGeM.	Discogem-2.0-a-parallel-corpus-of-english	https://aclanthology.org/2024.lrec-main.443/	
2024-01-01	2024	Christina Pollkläsener, Frances Yung, Ekaterina Lapshinova-Koltunski	Christina Pollkläsener, Frances Yung, Ekaterina Lapshinova-Koltunski (Across Languages and Cultures 2024)	manuscripts	Capturing variation of discourse relations in English parallel data through automatic annotation and alignment	Across Languages and Cultures	We present a study of discourse connectives and discourse relations in English parallel texts, i.e. in written and spoken originals, as well as translation and interpreting from German. For this, we apply automatic procedures to annotate discourse connectives and relations they trigger in a parallel corpus. We look at distributions of various connectives and discourse relations, comparing spoken and written mode, as well as original and translated or interpreted language production. Furthermore, we analyse the translation patterns in terms of translation entropy. We link our observations to the phenomena of explicitation and implicitation. We find that in both interpreting and translation, explicitation and implicitation patters are affected by the cognitive complexity of the discourse relation signalled by the connective. Moreover, we also show that the difference in the specificity of the same connectives in interpreting and translation also depends on the type of relation they trigger.	Capturing-variation-of-discourse-relations	https://akjournals.com/view/journals/084/25/2/article-p288.xml	
2025-01-01	2025	Frances Yung, Vera Demberg	Frances Yung, Vera Demberg (CoMeDi Workshop @ Coling 2025)	conferences	On Crowdsoursing Discourse Relation Annotation	Workshop on Navigating Disagreement in NLP Annotation @ COLING		On-crowdsoursing-task-design	https://aclanthology.org/2025.comedi-1.2/	
2025-01-01	2025	Frances Yung, Varsha Suresh, Zaynab Reza, Mansoor Ahmad, Vera Demberg	Frances Yung, Varsha Suresh, Zaynab Reza, Mansoor Ahmad, Vera Demberg (SIGDIAL 2025)	conferences	Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation Recognition 	SIGDIAL	Implicit discourse relation recognition (IDRR) -- the task of identifying the implicit coherence relation between two text spans -- requires deep semantic understanding. Recent studies have shown that zero- or few-shot approaches significantly lag behind supervised models, but LLMs may be useful for synthetic data augmentation, where LLMs generate a second argument following a specified coherence relation. We applied this approach in a cross-domain setting, generating discourse continuations using unlabelled target-domain data to adapt a base model which was trained on source-domain labelled data. Evaluations conducted on a large-scale test set revealed that different variations of the approach did not result in any significant improvements. We conclude that LLMs often fail to generate useful samples for IDRR, and emphasize the importance of considering both statistical significance and comparability when evaluating IDRR models.	Synthetic-data-augmentation	https://arxiv.org/abs/2503.20588	
2025-01-01	2025	Frances Yung, Vera Demberg	Frances Yung, Vera Demberg (UCCTS)	conferences	Meaning Shifts of Implicit Discourse Relations in Translation: a Multi-lingual Corpus Study	UCCTS		Meaning-Shifts-of-Implicit	https://www.uni-hildesheim.de/media/fb3/uebersetzungswissenschaft/UCCTS2025_BoA.pdf	
